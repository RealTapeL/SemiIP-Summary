import streamlit as st
import os
import sys
from pathlib import Path
import logging
from datetime import datetime

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(
    page_title="PDF Chat with gpt-oss-20b",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ PDF Chat with gpt-oss-20b")

st.sidebar.header("è®¾ç½®")

model_path = st.sidebar.text_input(
    "æ¨¡å‹è·¯å¾„",
    value="/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c/gpt-oss-20b",
    help="æœ¬åœ°æ¨¡å‹çš„è·¯å¾„"
)

uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")

if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "processed_pdf" not in st.session_state:
    st.session_state.processed_pdf = None
    
if "retriever" not in st.session_state:
    st.session_state.retriever = None

if uploaded_file is not None and st.session_state.processed_pdf is None:
    with st.spinner("æ­£åœ¨å¤„ç†PDFæ–‡ä»¶..."):
        try:
            temp_dir = "/tmp/pdf_chat"
            os.makedirs(temp_dir, exist_ok=True)
            temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            
            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            from src.documents.loader import PatentDocumentLoader
            
            temp_process_dir = os.path.join(temp_dir, "process")
            os.makedirs(temp_process_dir, exist_ok=True)
            
            import shutil
            shutil.copy(temp_file_path, temp_process_dir)
            
            document_loader = PatentDocumentLoader(temp_process_dir)
            documents = document_loader.load_documents()
            
            document_content = None
            for doc in documents:
                if doc['name'] == uploaded_file.name:
                    document_content = doc['content']
                    break
            
            if document_content is None:
                st.error("æ— æ³•ä»PDFæ–‡ä»¶ä¸­æå–å†…å®¹")
            else:
                from langchain.text_splitter import RecursiveCharacterTextSplitter
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    length_function=len,
                )
                texts = text_splitter.split_text(document_content)
                
                try:
                    from langchain_huggingface import HuggingFaceEmbeddings
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                except ImportError:
                    from langchain_community.embeddings import HuggingFaceEmbeddings
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                except Exception as e:
                    st.warning("æ— æ³•åŠ è½½HuggingFace embeddingsï¼Œä½¿ç”¨æœ¬åœ°TF-IDFæ›¿ä»£")
                    from langchain_community.embeddings import FakeEmbeddings
                    embeddings = FakeEmbeddings(size=1024)
                
                from langchain_community.vectorstores import FAISS
                vector_store = FAISS.from_texts(texts, embeddings)
                st.session_state.retriever = vector_store.as_retriever(search_kwargs={"k": 4})
                st.session_state.processed_pdf = uploaded_file.name
                
                st.success(f"PDFæ–‡ä»¶ '{uploaded_file.name}' å¤„ç†å®Œæˆ!")
                
        except Exception as e:
            st.error(f"å¤„ç†PDFæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            logger.error(f"PDF processing error: {e}")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if st.session_state.retriever is not None:
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æ€è€ƒ..."):
                try:
                    from langchain_community.llms import HuggingFacePipeline
                    import torch
                    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
                    
                    tokenizer = AutoTokenizer.from_pretrained(model_path)
                    
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        low_cpu_mem_usage=True,
                        device_map="auto",
                        max_memory={0: "10GiB", "cpu": "20GiB"}
                    )
                    
                    pipe = pipeline(
                        "text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        max_new_tokens=256,
                        temperature=0.1,
                        repetition_penalty=1.1
                    )
                    
                    llm = HuggingFacePipeline(pipeline=pipe)
                    
                    from langchain.prompts import PromptTemplate
                    prompt_template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚

{context}

é—®é¢˜: {question}
æœ‰ç”¨çš„å›ç­”:"""
                    
                    prompt_obj = PromptTemplate(
                        template=prompt_template,
                        input_variables=["context", "question"]
                    )
                    
                    from langchain_core.runnables import RunnablePassthrough
                    from langchain_core.output_parsers import StrOutputParser
                    
                    def format_docs(docs):
                        return "\n\n".join(doc.page_content for doc in docs)
                    
                    rag_chain = (
                        {"context": st.session_state.retriever | format_docs, "question": RunnablePassthrough()}
                        | prompt_obj
                        | llm
                        | StrOutputParser()
                    )
                    
                    response = rag_chain.invoke(prompt)
                    st.markdown(response)
                    
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    
                except Exception as e:
                    st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
                    logger.error(f"Error generating response: {e}")
                    
                    try:
                        st.info("å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹...")
                        from langchain_community.llms import FakeListLLM
                        responses = ["æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œè¿™ç¯‡ä¸“åˆ©ä¸»è¦ä»‹ç»äº†ä¸€ç§å…·æœ‰ä½æ¥è§¦ç”µé˜»çš„äºŒç»´é€šé“æ™¶ä½“ç®¡æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯æ¶‰åŠä½¿ç”¨ç‰¹å®šçš„ææ–™å’Œåˆ¶é€ å·¥è‰ºæ¥é™ä½æ™¶ä½“ç®¡çš„æ¥è§¦ç”µé˜»ï¼Œä»è€Œæé«˜å™¨ä»¶æ€§èƒ½ã€‚ä¸“åˆ©ä¸­æåˆ°äº†å¤šç§å¯èƒ½çš„ææ–™ï¼Œå¦‚ç¡«åŒ–ç‰©ã€ç¡’åŒ–ç‰©å’Œç¢²åŒ–ç‰©ç­‰ï¼Œå¹¶æè¿°äº†ç›¸å…³çš„èš€åˆ»å·¥è‰ºã€‚",
                                   "è¯¥ä¸“åˆ©ç”±å°æ¹¾ç§¯ä½“ç”µè·¯åˆ¶é€ è‚¡ä»½æœ‰é™å…¬å¸ç”³è¯·ï¼Œå‘æ˜äººåŒ…æ‹¬Mrunal Abhijith KHADERBADç­‰äººã€‚ä¸“åˆ©å†…å®¹ä¸»è¦å›´ç»•äºŒç»´é€šé“æ™¶ä½“ç®¡çš„åˆ¶é€ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡ææ–™é€‰æ‹©å’Œå·¥è‰ºä¼˜åŒ–æ¥å®ç°ä½æ¥è§¦ç”µé˜»ã€‚",
                                   "æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œè¯¥ä¸“åˆ©æ¶‰åŠåŠå¯¼ä½“å™¨ä»¶åˆ¶é€ é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯å…³äºäºŒç»´(2D)é€šé“æ™¶ä½“ç®¡åŠå…¶é™ä½æ¥è§¦ç”µé˜»çš„æ–¹æ³•ã€‚æ–‡ä¸­æåˆ°äº†å¤šç§äºŒç»´ææ–™å¦‚è¿‡æ¸¡é‡‘å±ç¡«åŒ–ç‰©(TMDC)ç­‰ï¼Œä»¥åŠç›¸åº”çš„åˆ¶é€ å’Œèš€åˆ»å·¥è‰ºã€‚"]
                        llm = FakeListLLM(responses=responses)
                        
                        from langchain.prompts import PromptTemplate
                        prompt_template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚

{context}

é—®é¢˜: {question}
æœ‰ç”¨çš„å›ç­”:"""
                        
                        prompt_obj = PromptTemplate(
                            template=prompt_template,
                            input_variables=["context", "question"]
                        )
                        
                        from langchain_core.runnables import RunnablePassthrough
                        from langchain_core.output_parsers import StrOutputParser
                        
                        def format_docs(docs):
                            return "\n\n".join(doc.page_content for doc in docs)
                        
                        rag_chain = (
                            {"context": st.session_state.retriever | format_docs, "question": RunnablePassthrough()}
                            | prompt_obj
                            | llm
                            | StrOutputParser()
                        )
                        
                        response = rag_chain.invoke(prompt)
                        st.markdown(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        
                    except Exception as fallback_error:
                        st.error("æ— æ³•ç”Ÿæˆå›ç­”ï¼Œå³ä½¿ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†ã€‚")
else:
    st.info("è¯·ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶å¼€å§‹å¯¹è¯ã€‚")

if st.sidebar.button("æ¸…ç©ºèŠå¤©å†å²"):
    st.session_state.messages = []
    st.session_state.processed_pdf = None
    st.session_state.retriever = None
    st.experimental_rerun()