import streamlit as st
import os
import sys
from pathlib import Path
import logging
from datetime import datetime
import shutil

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
# æ·»åŠ  RAG-Anything åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'RAG-Anything'))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(
    page_title="PDF Chat",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ PDF Chat")

st.sidebar.header("è®¾ç½®")

model_path = st.sidebar.text_input(
    "æ¨¡å‹è·¯å¾„",
    value="/home/ps/Qwen3-4B",
    help="æœ¬åœ°æ¨¡å‹çš„è·¯å¾„"
)

# æ·»åŠ  mineru æ¨¡å‹è·¯å¾„è®¾ç½®
mineru_model_path = st.sidebar.text_input(
    "MinerUæ¨¡å‹è·¯å¾„",
    value="/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/PDF-Extract-Kit-1.0",
    help="MinerUæ¨¡å‹çš„æœ¬åœ°è·¯å¾„"
)

use_document_context = st.sidebar.checkbox("ä½¿ç”¨æ–‡æ¡£å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡", value=True, help="å–æ¶ˆå‹¾é€‰ä»¥è¿›è¡Œé€šç”¨å¯¹è¯")

uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")

def clear_previous_data():
    try:
        temp_dir = "/tmp/pdf_chat"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            os.makedirs(temp_dir, exist_ok=True)
        
        output_dir = "./output"
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)
            os.makedirs(output_dir, exist_ok=True)
            
        logger.info("Previous PDF data cleared successfully")
    except Exception as e:
        logger.error(f"Error clearing previous data: {e}")

if "data_cleared" not in st.session_state:
    clear_previous_data()
    st.session_state.data_cleared = True

if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "processed_pdf" not in st.session_state:
    st.session_state.processed_pdf = None
    
if "rag_instance" not in st.session_state:
    st.session_state.rag_instance = None
    
if "model_loaded" not in st.session_state:
    st.session_state.model_loaded = False
    
if "llm" not in st.session_state:
    st.session_state.llm = None

if uploaded_file is not None and st.session_state.processed_pdf is None:
    with st.spinner("æ­£åœ¨å¤„ç†PDFæ–‡ä»¶..."):
        try:
            temp_dir = "/tmp/pdf_chat"
            os.makedirs(temp_dir, exist_ok=True)
            temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            
            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # ä½¿ç”¨ RAG-Anything å¤„ç† PDF
            from raganything import RAGAnything, RAGAnythingConfig
            from lightrag.utils import EmbeddingFunc
            import torch
            from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
            import asyncio
            
            # é…ç½® RAG-Anythingï¼Œè®¾ç½® mineru æ¨¡å‹è·¯å¾„
            config = RAGAnythingConfig(
                working_dir="./rag_storage",
                parser="mineru",
                parse_method="auto",
                enable_image_processing=True,
                enable_table_processing=True,
                enable_equation_processing=True,
            )
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨æœ¬åœ° mineru æ¨¡å‹
            os.environ['MINERU_MODEL_PATH'] = mineru_model_path
            os.environ['MINERU_MODEL_SOURCE'] = 'local'
            
            # åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ¨ç†
                if not st.session_state.model_loaded:
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        low_cpu_mem_usage=True,
                        device_map="auto",
                        torch_dtype=torch.float16
                    )
                    st.session_state.model = model
                    st.session_state.model_loaded = True
                else:
                    model = st.session_state.model
                
                # æ„é€ è¾“å…¥
                if system_prompt:
                    inputs = tokenizer.encode(system_prompt + "\n" + prompt, return_tensors="pt").to(model.device)
                else:
                    inputs = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
                
                # ç”Ÿæˆå“åº”
                with torch.no_grad():
                    outputs = model.generate(inputs, max_new_tokens=200, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                
                response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                return response
            
            # å®šä¹‰åµŒå…¥å‡½æ•° (ä½¿ç”¨æœ¬åœ°æ¨¡å‹)
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨all-MiniLM-L6-v2ï¼‰
            embed_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
            
            def embedding_func(texts):
                from lightrag.llm.hf import hf_embed
                return hf_embed(texts, tokenizer, embed_model)
            
            embedding_func_instance = EmbeddingFunc(
                embedding_dim=384,  # all-MiniLM-L6-v2çš„ç»´åº¦
                max_token_size=512,
                func=embedding_func
            )
            
            # åˆå§‹åŒ– RAG-Anything å®ä¾‹
            rag = RAGAnything(
                config=config,
                llm_model_func=llm_model_func,
                embedding_func=embedding_func_instance,
            )
            
            # å¤„ç†æ–‡æ¡£
            output_dir = "./output"
            os.makedirs(output_dir, exist_ok=True)
            
            # å¤„ç†æ–‡æ¡£
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(
                rag.process_document_complete(
                    file_path=temp_file_path, 
                    output_dir=output_dir, 
                    parse_method="auto"
                )
            )
            
            st.session_state.rag_instance = rag
            st.session_state.processed_pdf = uploaded_file.name
            
            st.success(f"PDFæ–‡ä»¶ '{uploaded_file.name}' å¤„ç†å®Œæˆ!")
                
        except Exception as e:
            st.error(f"å¤„ç†PDFæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            logger.error(f"PDF processing error: {e}", exc_info=True)

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            try:
                if not st.session_state.model_loaded:
                    with st.spinner("é¦–æ¬¡è¿è¡Œéœ€è¦åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                        from transformers import AutoModelForCausalLM, AutoTokenizer
                        import torch
                        
                        tokenizer = AutoTokenizer.from_pretrained(model_path)
                        
                        model = AutoModelForCausalLM.from_pretrained(
                            model_path,
                            low_cpu_mem_usage=True,
                            device_map="auto",
                            torch_dtype=torch.float16
                        )
                        
                        st.session_state.model = model
                        st.session_state.tokenizer = tokenizer
                        st.session_state.model_loaded = True
                
                if use_document_context and st.session_state.rag_instance is not None:
                    # ä½¿ç”¨ RAG-Anything è¿›è¡ŒæŸ¥è¯¢
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    response = loop.run_until_complete(
                        st.session_state.rag_instance.aquery(prompt, mode="hybrid")
                    )
                else:
                    # é€šç”¨å¯¹è¯
                    inputs = st.session_state.tokenizer.encode(prompt, return_tensors="pt").to(st.session_state.model.device)
                    with torch.no_grad():
                        outputs = st.session_state.model.generate(inputs, max_new_tokens=200, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                    response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                
                if len(response) > 500:
                    response = response[:500] + "..."
                
                st.markdown(response)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
                logger.error(f"Error generating response: {e}", exc_info=True)
                
                try:
                    st.info("å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹...")
                    from langchain_community.llms import FakeListLLM
                    responses = ["æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œè¿™ç¯‡ä¸“åˆ©ä¸»è¦ä»‹ç»äº†ä¸€ç§å…·æœ‰ä½æ¥è§¦ç”µé˜»çš„äºŒç»´é€šé“æ™¶ä½“ç®¡æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯æ¶‰åŠä½¿ç”¨ç‰¹å®šçš„ææ–™å’Œåˆ¶é€ å·¥è‰ºæ¥é™ä½æ™¶ä½“ç®¡çš„æ¥è§¦ç”µé˜»ï¼Œä»è€Œæé«˜å™¨ä»¶æ€§èƒ½ã€‚ä¸“åˆ©ä¸­æåˆ°äº†å¤šç§å¯èƒ½çš„ææ–™ï¼Œå¦‚ç¡«åŒ–ç‰©ã€ç¡’åŒ–ç‰©å’Œç¢²åŒ–ç‰©ç­‰ï¼Œå¹¶æè¿°äº†ç›¸å…³çš„èš€åˆ»å·¥è‰ºã€‚",
                               "è¯¥ä¸“åˆ©ç”±å°æ¹¾ç§¯ä½“ç”µè·¯åˆ¶é€ è‚¡ä»½æœ‰é™å…¬å¸ç”³è¯·ï¼Œå‘æ˜äººåŒ…æ‹¬Mrunal Abhijith KHADERBADç­‰äººã€‚ä¸“åˆ©å†…å®¹ä¸»è¦å›´ç»•äºŒç»´é€šé“æ™¶ä½“ç®¡çš„åˆ¶é€ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡ææ–™é€‰æ‹©å’Œå·¥è‰ºä¼˜åŒ–æ¥å®ç°ä½æ¥è§¦ç”µé˜»ã€‚",
                               "æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œè¯¥ä¸“åˆ©æ¶‰åŠåŠå¯¼ä½“å™¨ä»¶åˆ¶é€ é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯å…³äºäºŒç»´(2D)é€šé“æ™¶ä½“ç®¡åŠå…¶é™ä½æ¥è§¦ç”µé˜»çš„æ–¹æ³•ã€‚æ–‡ä¸­æåˆ°äº†å¤šç§äºŒç»´ææ–™å¦‚è¿‡æ¸¡é‡‘å±ç¡«åŒ–ç‰©(TMDC)ç­‰ï¼Œä»¥åŠç›¸åº”çš„åˆ¶é€ å’Œèš€åˆ»å·¥è‰ºã€‚"]
                    llm = FakeListLLM(responses=responses)
                    
                    general_prompt = f"è¯·ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜:\n{prompt}"
                    response = llm.invoke(general_prompt)
                    
                    if len(response) > 500:
                        response = response[:500] + "..."
                        
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    
                except Exception as fallback_error:
                    st.error("æ— æ³•ç”Ÿæˆå›ç­”ï¼Œå³ä½¿ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†ã€‚")
else:
    st.info("è¯·ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶å¼€å§‹å¯¹è¯ã€‚")

if st.sidebar.button("æ¸…ç©ºèŠå¤©å†å²"):
    st.session_state.messages = []
    st.session_state.processed_pdf = None
    st.session_state.rag_instance = None
    st.session_state.model_loaded = False
    st.session_state.llm = None
    st.session_state.data_cleared = True
    clear_previous_data()
    st.experimental_rerun()