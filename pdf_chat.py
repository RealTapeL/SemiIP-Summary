import streamlit as st
import os
import sys
from pathlib import Path
import logging
from datetime import datetime
import shutil

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'RAG-Anything'))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(
    page_title="PDF Chat",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ PDF Chat")

st.sidebar.header("è®¾ç½®")

model_path = st.sidebar.text_input(
    "æ¨¡å‹è·¯å¾„",
    value="/home/ps/Qwen3-4B",
    help="æœ¬åœ°æ¨¡å‹çš„è·¯å¾„"
)

mineru_model_path = st.sidebar.text_input(
    "MinerUæ¨¡å‹è·¯å¾„",
    value="/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/PDF-Extract-Kit-1.0",
    help="MinerUæ¨¡å‹çš„æœ¬åœ°è·¯å¾„"
)

use_document_context = st.sidebar.checkbox("ä½¿ç”¨æ–‡æ¡£å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡", value=True, help="å–æ¶ˆå‹¾é€‰ä»¥è¿›è¡Œé€šç”¨å¯¹è¯")

query_timeout = st.sidebar.slider("æŸ¥è¯¢è¶…æ—¶æ—¶é—´(ç§’)", min_value=30, max_value=600, value=180, step=10)

uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")

def clear_previous_data():
    try:
        temp_dir = "/tmp/pdf_chat"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            os.makedirs(temp_dir, exist_ok=True)
        
        output_dir = "./output"
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)
            os.makedirs(output_dir, exist_ok=True)
            
        rag_storage_dir = "./rag_storage"
        if os.path.exists(rag_storage_dir):
            shutil.rmtree(rag_storage_dir)
            os.makedirs(rag_storage_dir, exist_ok=True)
            
        logger.info("Previous PDF data cleared successfully")
    except Exception as e:
        logger.error(f"Error clearing previous data: {e}")

if "data_cleared" not in st.session_state:
    clear_previous_data()
    st.session_state.data_cleared = True

if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "processed_pdf" not in st.session_state:
    st.session_state.processed_pdf = None
    
if "rag_instance" not in st.session_state:
    st.session_state.rag_instance = None
    
if "model_loaded" not in st.session_state:
    st.session_state.model_loaded = False
    
if "llm" not in st.session_state:
    st.session_state.llm = None

@st.cache_resource
def load_models(_model_path, _mineru_model_path):
    st.info("æ­£åœ¨é¢„åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    try:
        os.environ['MINERU_MODEL_PATH'] = _mineru_model_path
        os.environ['MINERU_MODEL_SOURCE'] = 'local'
        
        from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, BitsAndBytesConfig
        import torch
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
        progress_bar.progress(10)
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        tokenizer = AutoTokenizer.from_pretrained(_model_path)
        status_text.text("æ­£åœ¨åŠ è½½ä¸»æ¨¡å‹...")
        progress_bar.progress(30)
        
        model = AutoModelForCausalLM.from_pretrained(
            _model_path,
            low_cpu_mem_usage=True,
            device_map="cuda",
            torch_dtype=torch.float16,
            quantization_config=bnb_config,
        )
        status_text.text("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
        progress_bar.progress(70)
        
        embed_model = AutoModel.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
        embed_tokenizer = AutoTokenizer.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
        
        progress_bar.progress(100)
        status_text.text("æ¨¡å‹åŠ è½½å®Œæˆ!")
        
        st.success("æ¨¡å‹é¢„åŠ è½½å®Œæˆ!")
        return tokenizer, model, embed_tokenizer, embed_model
    except Exception as e:
        st.error(f"æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {str(e)}")
        logger.error(f"Model loading error: {e}", exc_info=True)
        return None, None, None, None

if not st.session_state.model_loaded and model_path and mineru_model_path:
    with st.spinner("æ­£åœ¨é¢„åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´"):
        tokenizer, model, embed_tokenizer, embed_model = load_models(model_path, mineru_model_path)
        if model is not None:
            st.session_state.model = model
            st.session_state.tokenizer = tokenizer
            st.session_state.embed_tokenizer = embed_tokenizer
            st.session_state.embed_model = embed_model
            st.session_state.model_loaded = True

if uploaded_file is not None and st.session_state.processed_pdf is None:
    with st.spinner("æ­£åœ¨å¤„ç†PDFæ–‡ä»¶..."):
        try:
            temp_dir = "/tmp/pdf_chat"
            os.makedirs(temp_dir, exist_ok=True)
            temp_file_path = os.path.join(temp_dir, uploaded_file.name)
            
            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            from raganything import RAGAnything, RAGAnythingConfig
            from lightrag.utils import EmbeddingFunc
            import torch
            from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, BitsAndBytesConfig
            import asyncio
            
            config = RAGAnythingConfig(
                working_dir="./rag_storage",
                parser="mineru",
                parse_method="auto",
                enable_image_processing=True,
                enable_table_processing=True,
                enable_equation_processing=True,
                context_window=3,
                max_context_tokens=3000,
            )
            
            if not st.session_state.model_loaded:
                st.info("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
                os.environ['MINERU_MODEL_PATH'] = mineru_model_path
                os.environ['MINERU_MODEL_SOURCE'] = 'local'
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                status_text.text("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
                progress_bar.progress(10)
                
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
                
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                status_text.text("æ­£åœ¨åŠ è½½ä¸»æ¨¡å‹...")
                progress_bar.progress(30)
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    low_cpu_mem_usage=True,
                    device_map="cuda",
                    torch_dtype=torch.float16,
                    quantization_config=bnb_config,
                )
                status_text.text("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
                progress_bar.progress(70)
                
                embed_model = AutoModel.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
                embed_tokenizer = AutoTokenizer.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
                
                progress_bar.progress(100)
                status_text.text("æ¨¡å‹åŠ è½½å®Œæˆ!")
                
                st.session_state.model = model
                st.session_state.tokenizer = tokenizer
                st.session_state.model_loaded = True
            else:
                tokenizer = st.session_state.tokenizer
                model = st.session_state.model
            
            async def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
                if system_prompt:
                    full_prompt = system_prompt + "\n" + prompt
                else:
                    full_prompt = prompt
                    
                inputs = tokenizer.encode(full_prompt, return_tensors="pt", truncation=True, max_length=2048).to(model.device)
                
                with torch.no_grad():
                    outputs = model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                
                response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                return response
            
            if "embed_model" not in st.session_state:
                embed_model = AutoModel.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
                embed_tokenizer = AutoTokenizer.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
                st.session_state.embed_model = embed_model
                st.session_state.embed_tokenizer = embed_tokenizer
            else:
                embed_model = st.session_state.embed_model
                embed_tokenizer = st.session_state.embed_tokenizer
            
            def embedding_func(texts):
                from lightrag.llm.hf import hf_embed
                return hf_embed(texts, embed_tokenizer, embed_model)
            
            embedding_func_instance = EmbeddingFunc(
                embedding_dim=384,
                max_token_size=512,
                func=embedding_func
            )
            
            rag = RAGAnything(
                config=config,
                llm_model_func=llm_model_func,
                embedding_func=embedding_func_instance,
            )
            
            output_dir = "./output"
            os.makedirs(output_dir, exist_ok=True)
            
            st.info("å¼€å§‹è§£æPDFæ–‡æ¡£ï¼Œè¯·ç¨å€™...")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(
                rag.process_document_complete(
                    file_path=temp_file_path, 
                    output_dir=output_dir, 
                    parse_method="auto"
                )
            )
            
            st.session_state.rag_instance = rag
            st.session_state.processed_pdf = uploaded_file.name
            
            st.success(f"PDFæ–‡ä»¶ '{uploaded_file.name}' å¤„ç†å®Œæˆ!")
                
        except Exception as e:
            st.error(f"å¤„ç†PDFæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            logger.error(f"PDF processing error: {e}", exc_info=True)

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            try:
                if not st.session_state.model_loaded:
                    with st.spinner("é¦–æ¬¡è¿è¡Œéœ€è¦åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
                        import torch
                        
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        status_text.text("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
                        progress_bar.progress(10)
                        
                        bnb_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4",
                            bnb_4bit_compute_dtype=torch.bfloat16,
                        )
                        
                        tokenizer = AutoTokenizer.from_pretrained(model_path)
                        status_text.text("æ­£åœ¨åŠ è½½ä¸»æ¨¡å‹...")
                        progress_bar.progress(30)
                        
                        model = AutoModelForCausalLM.from_pretrained(
                            model_path,
                            low_cpu_mem_usage=True,
                            device_map="cuda",
                            torch_dtype=torch.float16,
                            quantization_config=bnb_config,
                        )
                        status_text.text("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
                        progress_bar.progress(70)
                        
                        embed_model = AutoModel.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
                        embed_tokenizer = AutoTokenizer.from_pretrained("/media/ps/4be23142-02e1-4581-90e2-3316bdb6f49c1/SemiIP-Summary/all-MiniLM-L6-v2")
                        
                        progress_bar.progress(100)
                        status_text.text("æ¨¡å‹åŠ è½½å®Œæˆ!")
                        
                        st.session_state.model = model
                        st.session_state.tokenizer = tokenizer
                        st.session_state.model_loaded = True
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡å¹¶ä¸”RAGå®ä¾‹å­˜åœ¨
                if use_document_context and st.session_state.rag_instance is not None:
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        # é¦–å…ˆå°è¯•ä½¿ç”¨naiveæ¨¡å¼ç›´æ¥æŸ¥è¯¢æ–‡æ¡£å†…å®¹
                        response = loop.run_until_complete(
                            asyncio.wait_for(
                                st.session_state.rag_instance.aquery(prompt, mode="naive"),
                                timeout=float(query_timeout)
                            )
                        )
                        # å¦‚æœnaiveæ¨¡å¼æ²¡æœ‰è¿”å›ç»“æœæˆ–ç»“æœå¤ªçŸ­
                        if not response or len(response.strip()) < 10:
                            st.info("ç›´æ¥æ–‡æ¡£æŸ¥è¯¢æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œå°è¯•çŸ¥è¯†å›¾è°±æŸ¥è¯¢...")
                            try:
                                response = loop.run_until_complete(
                                    asyncio.wait_for(
                                        st.session_state.rag_instance.aquery(prompt, mode="hybrid"),
                                        timeout=float(query_timeout)
                                    )
                                )
                            except:
                                # å¦‚æœçŸ¥è¯†å›¾è°±æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•è·å–åŸå§‹æ–‡æ¡£å†…å®¹
                                pass
                        
                        # å¦‚æœä»ç„¶æ²¡æœ‰ç»“æœï¼Œç›´æ¥ä½¿ç”¨æ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”
                        if not response or len(response.strip()) < 10:
                            st.info("ä½¿ç”¨æ–‡æ¡£å†…å®¹ç›´æ¥ç”Ÿæˆå›ç­”...")
                            try:
                                # è·å–æ–‡æ¡£å†…å®¹
                                full_docs_storage = st.session_state.rag_instance.lightrag.full_docs
                                doc_keys = loop.run_until_complete(full_docs_storage.get_keys())
                                context_content = ""
                                for key in doc_keys[:1]:  # åªå–ç¬¬ä¸€ä¸ªæ–‡æ¡£
                                    doc_content = loop.run_until_complete(full_docs_storage.get_by_id(key))
                                    if doc_content and 'content' in doc_content:
                                        context_content = doc_content['content']
                                        break
                                
                                if context_content:
                                    # æ„é€ åŒ…å«æ–‡æ¡£å†…å®¹çš„æç¤º
                                    full_prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š\n\n{context_content[:4000]}\n\né—®é¢˜ï¼š{prompt}\n\nè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"
                                    inputs = st.session_state.tokenizer.encode(full_prompt, return_tensors="pt", truncation=True, max_length=2048).to(st.session_state.model.device)
                                    with torch.no_grad():
                                        outputs = st.session_state.model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                                    response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                                else:
                                    # å¦‚æœæ²¡æœ‰æ–‡æ¡£å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨LLM
                                    st.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”...")
                                    inputs = st.session_state.tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=2048).to(st.session_state.model.device)
                                    with torch.no_grad():
                                        outputs = st.session_state.model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                                    response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                            except Exception as context_error:
                                logger.error(f"ä½¿ç”¨æ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {context_error}")
                                # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
                                st.info("ç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”...")
                                inputs = st.session_state.tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=2048).to(st.session_state.model.device)
                                with torch.no_grad():
                                    outputs = st.session_state.model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                                response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                    except asyncio.TimeoutError:
                        # å¦‚æœæ··åˆæ¨¡å¼è¶…æ—¶ï¼Œå°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å¼
                        try:
                            st.info("æ··åˆæ¨¡å¼è¶…æ—¶ï¼Œå°è¯•ä½¿ç”¨ç®€åŒ–çš„æ£€ç´¢æ¨¡å¼...")
                            response = loop.run_until_complete(
                                asyncio.wait_for(
                                    st.session_state.rag_instance.aquery(prompt, mode="naive"),
                                    timeout=float(query_timeout)
                                )
                            )
                            if not response or len(response.strip()) == 0:
                                st.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”...")
                                inputs = st.session_state.tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=2048).to(st.session_state.model.device)
                                with torch.no_grad():
                                    outputs = st.session_state.model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                                response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                        except asyncio.TimeoutError:
                            # å¦‚æœä»ç„¶è¶…æ—¶ï¼Œä½¿ç”¨ç›´æ¥çš„LLMç”Ÿæˆ
                            st.info("æ£€ç´¢æ¨¡å¼ä¹Ÿè¶…æ—¶ï¼Œç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”...")
                            inputs = st.session_state.tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=2048).to(st.session_state.model.device)
                            with torch.no_grad():
                                outputs = st.session_state.model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                            response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                else:
                    # å³ä½¿ä¸ä½¿ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œä¹Ÿä¸å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯
                    inputs = st.session_state.tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=2048).to(st.session_state.model.device)
                    with torch.no_grad():
                        outputs = st.session_state.model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                    response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                
                if len(response) > 1000:
                    response = response[:1000] + "..."
                
                st.markdown(response)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
                logger.error(f"Error generating response: {e}", exc_info=True)
                
                try:
                    st.info("ä½¿ç”¨ç›´æ¥å¯¹è¯æ¨¡å¼...")
                    # ç›´æ¥ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼Œå°†PDFå†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
                    if st.session_state.rag_instance is not None and hasattr(st.session_state.rag_instance, 'lightrag'):
                        # å°è¯•è·å–æ–‡æ¡£å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
                        try:
                            # è·å–æ–‡æ¡£å†…å®¹
                            full_docs_storage = st.session_state.rag_instance.lightrag.full_docs
                            doc_keys = loop.run_until_complete(full_docs_storage.get_keys())
                            context_content = ""
                            for key in doc_keys[:3]:  # é™åˆ¶å‰3ä¸ªæ–‡æ¡£
                                doc_content = loop.run_until_complete(full_docs_storage.get_by_id(key))
                                if doc_content and 'content' in doc_content:
                                    context_content += doc_content['content'] + "\n"
                            
                            if context_content:
                                full_prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š\n\n{context_content[:4000]}\n\né—®é¢˜ï¼š{prompt}"
                            else:
                                full_prompt = prompt
                        except Exception as context_error:
                            logger.error(f"è·å–æ–‡æ¡£ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {context_error}")
                            full_prompt = prompt
                    else:
                        full_prompt = prompt
                    
                    inputs = st.session_state.tokenizer.encode(full_prompt, return_tensors="pt", truncation=True, max_length=2048).to(st.session_state.model.device)
                    with torch.no_grad():
                        outputs = st.session_state.model.generate(inputs, max_new_tokens=500, temperature=0.1, repetition_penalty=1.2, do_sample=False)
                    response = st.session_state.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
                    
                    if len(response) > 1000:
                        response = response[:1000] + "..."
                        
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    
                except Exception as fallback_error:
                    st.error("æ— æ³•ç”Ÿæˆå›ç­”ï¼Œå³ä½¿ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†ã€‚")
else:
    st.info("è¯·ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶å¼€å§‹å¯¹è¯ã€‚")

if st.sidebar.button("æ¸…ç©ºèŠå¤©å†å²"):
    st.session_state.messages = []
    st.session_state.processed_pdf = None
    st.session_state.rag_instance = None
    st.session_state.model_loaded = False
    st.session_state.llm = None
    st.session_state.data_cleared = True
    clear_previous_data()
    st.experimental_rerun()